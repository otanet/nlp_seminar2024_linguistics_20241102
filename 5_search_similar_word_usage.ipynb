{"cells":[{"cell_type":"markdown","metadata":{"id":"8izrmpAhVtl_"},"source":["コーパスを読み込み対象単語（bank）を含む文を抽出"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_bCH_ibolKs","executionInfo":{"status":"ok","timestamp":1724830266026,"user_tz":-540,"elapsed":2672,"user":{"displayName":"Ryo Nagata","userId":"04273246616371283600"}},"outputId":"52c64bcd-ce97-4fc6-fc07-6420f9ebc34e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","He drew some money from the bank.\n","John got a bank transfer form to make a bank transfer.\n","My father had worked as a bank clerk for a long time.\n","Someone raided a bank.\n","It functions as a data bank.\n","They walked along the river bank.\n","The city stands on the right bank of the Saine.\n","There is a sand bank between the two towns.\n","They sat down against the bank by the wayside.\n","The heavy rain broke the bank.\n"]}],"source":["import google.colab.drive\n","google.colab.drive.mount('/content/drive/')\n","\n","sentences_in_corpus = []\n","with open('/content/drive/MyDrive/nlpseminar2024/simple_corpus.txt') as f:\n","  for data_line in f:\n","    data_line = data_line.rstrip()\n","    sentences_in_corpus.append(data_line)\n","\n","# 確認用に出力\n","for sentence in sentences_in_corpus:\n","    print(sentence)"]},{"cell_type":"markdown","metadata":{"id":"2D-c_ak6pTU5"},"source":["BERTの準備（BERTを使用するための定型処理）"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2934,"status":"ok","timestamp":1724830268956,"user":{"displayName":"Ryo Nagata","userId":"04273246616371283600"},"user_tz":-540},"id":"2x0a4sI2pVUc","outputId":"94ab3a6a-7766-4b6d-8a75-077e13856c5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Working on: cuda:0\n"]}],"source":["!pip install transformers\n","import torch\n","import transformers\n","from transformers import AutoTokenizer\n","from transformers import BertModel\n","\n","model_name = 'bert-base-cased' # 'bert-base-uncased' to ignore difference in upper/lower cases\n","bert_model = BertModel.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # CPU mode or GPU\n","bert_model.to(device)\n","bert_model.eval()\n","print('Working on:', device)"]},{"cell_type":"markdown","metadata":{"id":"qEIRDf9f1WWm"},"source":["対象文中の全単語をベクトル化"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":330,"status":"ok","timestamp":1724830401971,"user":{"displayName":"Ryo Nagata","userId":"04273246616371283600"},"user_tz":-540},"id":"WaFGyJXG1ZDJ","outputId":"0d7af62a-c9a7-4dcc-e97f-5b835747bfd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda:0\n"]}],"source":["import copy\n","vectors_with_indices = []\n","print('Device:', device)\n","for sentence in sentences_in_corpus:\n","  # token to ids\n","  tokenized = tokenizer(sentence,\n","                        return_tensors='pt', padding=True, truncation=True)\n","  token_ids = tokenized['input_ids'].to(device)\n","  mask_ids = tokenized['attention_mask'].to(device)\n","\n","  bert_output = bert_model(token_ids, mask_ids)                                   # token（単語をベクトルに変換）\n","\n","  tokens = tokenizer.tokenize(sentence)\n","  last_hidden_state = bert_output.last_hidden_state[0]                            # 最終層の出力（ベクトル）を利用（他の層や平均をとってもよい）\n","  # 0番目は文頭トークン[CLS]のため除外．1番目からスタート\n","  for token_index in range(1, len(last_hidden_state)):\n","    token_vector  = last_hidden_state[token_index]\n","    token_vector = token_vector.to('cpu').detach().numpy().copy()                 # GPUからCPUへ\n","    vectors_with_indices.append([tokens,        #分割結果\n","                                 token_index-1, #単語番号（何番目の単語か）\n","                                 token_vector]) #単語ベクトル"]},{"cell_type":"markdown","metadata":{"id":"3S3zltH-jDCH"},"source":["対象文中の対象フレーズの位置を見つける処理"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUyMvVIhg24l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724830411056,"user_tz":-540,"elapsed":299,"user":{"displayName":"Ryo Nagata","userId":"04273246616371283600"}},"outputId":"8873b3a1-71eb-48d5-a3d0-1b1b39e97226"},"outputs":[{"output_type":"stream","name":"stdout","text":["Target word: bank\n","['I', 'drew', 'some', 'bucks', 'from', 'the', 'bank', '.']\n","[(6, 7)] I drew some bucks from the bank.\n"]}],"source":["# 次の三行（3-5）は対象フレーズを見つけるためのプログラムの準備\n","# util.pyの中に具体的な処理が書かれている\n","import sys\n","sys.path.append('drive/My Drive/nlpseminar2024')\n","import util\n","\n","#target_sentence = 'There is a large river bank.'\n","target_sentence = 'I drew some bucks from the bank.'\n","target_word = 'bank'  # 分析対象としてbankを指定\n","\n","print('Target word:', target_word)\n","tokens_of_target_word =  tokenizer.tokenize(target_word)                          # 一単語でも複数のサブワードに分割される可能性あり\n","\n","target_words = tokenizer.tokenize(target_sentence)\n","print(target_words)\n","# (何番目の単語から，何番目の単語までか)\n","target_span = util.find_target_spans(target_words, tokens_of_target_word)\n","print(target_span, target_sentence)"]},{"cell_type":"markdown","metadata":{"id":"ziOgmDQVp0J9"},"source":["対象単語をベクトルに変換"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQfRoyy6wgVK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724830459770,"user_tz":-540,"elapsed":327,"user":{"displayName":"Ryo Nagata","userId":"04273246616371283600"}},"outputId":"61b42e5d-3ac0-47a2-d3b8-88aebd07989f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.51508087  0.09701937 -0.08343408  0.23143525  0.13654947  0.07786781\n","  0.01467468 -0.38582292 -0.5197119  -0.00053196]\n"]}],"source":["tokenized = tokenizer(target_sentence,\n","                      return_tensors='pt', padding=True, truncation=True)\n","token_ids = tokenized['input_ids'].to(device)\n","mask_ids = tokenized['attention_mask'].to(device)\n","\n","# BERTに入力しベクトルに変換\n","bert_output = bert_model(token_ids, mask_ids)\n","\n","# BERTの出力は文頭に特殊トークン[CLS]（文頭トークン）を含むため\n","# indexが一つずれることに注意（begin_index + 1）\n","begin_index, end_index = target_span[0]\n","token_vectors = bert_output.last_hidden_state[0, begin_index+1:end_index+1]\n","\n","# 固定サイズにするために平均をとる\n","mean_token_vector = torch.mean(token_vectors, dim=0)\n","\n","# Numpyのベクトルに型変換\n","target_vector = mean_token_vector.to('cpu').detach().numpy().copy()\n","print(target_vector[0:10]) # ベクトルの0～9番目の値を表示"]},{"cell_type":"markdown","metadata":{"id":"ddhXDGgfqNPu"},"source":["対象フレーズのベクトルとコーパス中の全単語に対するベクトル間の類似度を計算"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3U3qq8_Pvjdt"},"outputs":[],"source":["import numpy as np  # ベクトル，行列計算モジュール\n","similarities = []\n","norm_of_target_vec = np.linalg.norm(target_vector)                                # 対象単語のベクトルのノルム（長さ）\n","for tokens, token_index, token_vector in vectors_with_indices:\n","  product =  np.dot(token_vector, target_vector) # 二つのベクトルの内積\n","  norm = np.linalg.norm(token_vector) # 類似度計算対象の単語のベクトルのノルム\n","  cos_sim =  product/(norm_of_target_vec*norm)   # 余弦類似度\n","  similarities.append((cos_sim, tokens, token_index))\n","\n","sorted_sims = sorted(similarities, key=lambda x: x[0], reverse=True)              # 類似度の高い順にソート"]},{"cell_type":"markdown","metadata":{"id":"dDDorRI9xEJT"},"source":["結果の表示"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucegnKQQr5v4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724830474770,"user_tz":-540,"elapsed":318,"user":{"displayName":"Ryo Nagata","userId":"04273246616371283600"}},"outputId":"ba07778e-4f25-4f7f-d119-3f1bcfde1982"},"outputs":[{"output_type":"stream","name":"stdout","text":["------\n","Target word: bank\n","Target sentence: I drew some bucks from the bank.\n","------\n","0.97155577 He drew some money from the *bank* .\n","0.90962034 Someone raided a *bank* .\n","0.8676604 My father had worked as a *bank* clerk for a long time .\n","0.8359408 John got a *bank* transfer form to make a bank transfer .\n","0.81758964 John got a bank transfer form to make a *bank* transfer .\n"]}],"source":["print('------')\n","print('Target word:', target_word)\n","print('Target sentence:', target_sentence)\n","print('------')\n","\n","top_n = 5 # 上位5件\n","tag = '*' # 検索された語をわかりやすく表示するためのタグ\n","for cos_sim, tokens, token_index in sorted_sims[:top_n]:\n","  output_tokens = tokens[:]\n","  output_tokens[token_index] = tag + output_tokens[token_index] + tag\n","  sentence_for_output = ' '.join(output_tokens)\n","\n","  print(cos_sim, sentence_for_output)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1vXEqiiZJcn0RW_sIiL33eakO0uJXlElS","timestamp":1692190529044},{"file_id":"1SL7jtWIk3ZpwJkLthKTuoGGf7fqsY4a1","timestamp":1691928531034},{"file_id":"1ulnm-JNRjAv7jH6HAolQNWFE_g3L8LsJ","timestamp":1691927381065},{"file_id":"1iRI4ysonjbP2x60BPcgKFdy_HIAfUBGw","timestamp":1691671618639}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}